# -*- coding: utf-8 -*-
"""Spell & Grammar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xyIIVHLg9ZvB_ezknNuRGtjKEooGMLHC
"""

!pip install sinling transformers

from sinling import SinhalaTokenizer

# Initialize Sinhala tokenizer
tokenizer = SinhalaTokenizer()

# Define the spell correction dictionary
spell_dict = {
    "අයියි": "අයියා",
"කොහෙද":"කොහේද",
"පසලට":"පාසලට",
"ගරින්":"ගෙදරින්",
"අසනපයක්": "අසනීපයක්",
"නැරබුවෙමි":"නැරඹුවෙමි",
"පසදින": "පසුදින",
"රහලට": "රෝහලට",

}

# Spell correction function
def sinhala_spell_corrector(paragraph, spell_dict, max_corrections=5):
    tokens = tokenizer.tokenize(paragraph)
    corrections = 0
    corrected_tokens = []

    for token in tokens:
        if token in spell_dict and corrections < max_corrections:
            corrected_tokens.append(spell_dict[token])
            corrections += 1
        else:
            corrected_tokens.append(token)

    corrected_paragraph = ' '.join(corrected_tokens)
    return corrected_paragraph, corrections

# Example usage
paragraph = "අසනපයක් තිබුනු නිසා මම ගරින් පසලට නොගියෙමි. අයියි පසදින මා රෝහලට රැගෙන ගියේය."
corrected_paragraph, corrections = sinhala_spell_corrector(paragraph, spell_dict)

print("Original Paragraph:", paragraph)
print("Corrected Paragraph:", corrected_paragraph)
print("Total Corrections:", corrections)

"""Approach 1: Rule-Based Grammar Checker

"""

# Rule-based grammar correction
def rule_based_grammar_correction(text):
    corrections = {
        "ඔවුහු පාසලට යනවා": "ඔවුහු පාසලට යති",  # Subject-Verb Agreement Error
        "මම යන්න ඕන කාර්යාලයට": "මම කාර්යාලයට යන්න ඕන.",  # Word Order Error
        "අපි ගෙදර යනවා": "අපි ගෙදර යමු",
        "මට  බලන්න ඕන එය": "මට එය බලන්න ඕනේ.",
        "ඔවුන් කාර්යාලයට ගිහින්": "ඔවුන් කාර්යාලයට ගියෝය"
    }
    if text in corrections:
        return corrections[text]
    return text

# Test Sentences
test_sentences = [
    "ඔවුහු පාසලට යනවා",  # Subject-Verb Agreement Error
    "මම යන්න ඕන කාර්යාලයට",  # Word Order Error
]

# Test the grammar correction function
for sentence in test_sentences:
    corrected_sentence = rule_based_grammar_correction(sentence)
    print(f"Original: {sentence}")
    print(f"Corrected: {corrected_sentence}")

"""Approach 2: Deep Learning Model (LSTM)

"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split

# Sample dataset (Sentences and Labels)
data = [
    ("ඔවුහු පාසලට යනවා", 1),  # Incorrect
    ("ඔවුහු පාසලට යති", 0),  # Correct
    ("මම යන්න ඕන කාර්යාලයට", 1),  # Incorrect
    ("මම කාර්යාලයට යන්න ඕන.", 0),  # Correct
    ("අපි යමු", 0),  # Correct
    ("අපි යනවා", 1),  # Incorrect
]

# Separate sentences and labels
sentences, labels = zip(*data)
labels = np.array(labels)

# Tokenize sentences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Pad sequences
max_len = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding="post")

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),
    LSTM(128, return_sequences=False),
    Dropout(0.5),
    Dense(64, activation="relu"),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

# Compile the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Train the model
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=16)

# Test the model
def test_lstm_model(text):
    seq = tokenizer.texts_to_sequences([text])
    padded_seq = pad_sequences(seq, maxlen=max_len, padding="post")
    prediction = model.predict(padded_seq)
    return "Incorrect Grammar" if prediction[0] > 0.5 else "Correct Grammar"

# Example sentences for testing
test_sentences = [
    "ඔවුහු පාසලට යනවා",  # Incorrect
    "ඔවුහු පාසලට යති",    # Correct
    "මම කාර්යාලයට යන්න ඕන",  # Correct
    "අපි යමු",  # Correct

]

# Test the model
print("\nTesting Sentences:")
for sentence in test_sentences:
    result = test_lstm_model(sentence)
    print(f"Sentence: {sentence} => Prediction: {result}")

"""Approach 3: Pre-trained mBERT (Transformer-based NLP)"""

from google.colab import files

# Upload the dataset
uploaded = files.upload()

# Confirm the upload
for filename in uploaded.keys():
    print(f"Uploaded {filename}")
# Install required libraries (if not already installed)
# !pip install transformers torch huggingface_hub

# Import necessary libraries
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
import torch
from torch.utils.data import Dataset

# Dataset file name
data_file = "sinhala_grammar_dataset.txt"

# Process dataset
sentences = []
labels = []

with open(data_file, "r", encoding="utf-8") as f:
    lines = f.readlines()

# Parse dataset
for line in lines[1:]:  # Skip the header line
    line = line.strip()
    if "|" in line:
        try:
            sentence, label = line.split("|")
            sentences.append(sentence)
            labels.append(int(label))
        except ValueError:
            print(f"Skipping malformed line: {line}")
    else:
        print(f"Skipping malformed line: {line}")

# Ensure dataset integrity
assert len(sentences) == len(labels), "Mismatch between sentences and labels!"

# Split data into training and validation sets
train_size = int(0.8 * len(sentences))
train_sentences, val_sentences = sentences[:train_size], sentences[train_size:]
train_labels, val_labels = labels[:train_size], labels[train_size:]

# Load mBERT tokenizer and model
model_name = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Create dataset class
class SinhalaGrammarDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_length):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = str(self.sentences[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            sentence,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt",
            return_attention_mask=True
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# Create dataset splits
train_dataset = SinhalaGrammarDataset(train_sentences, train_labels, tokenizer, max_length=128)
val_dataset = SinhalaGrammarDataset(val_sentences, val_labels, tokenizer, max_length=128)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    learning_rate=5e-5,
    logging_dir="./logs",
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)

# Train the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./sinhala_grammar_model")
tokenizer.save_pretrained("./sinhala_grammar_model")

# Testing data
test_sentences = [
    "ඔවුන් යති.",  # Correct
    "ඔවුන් යනවා.",  # Incorrect
    "මම කාර්යාලයට යන්න ඕන.",  # Correct
    "මම යන්න ඕන කාර්යාලයට.",  # Incorrect
]
test_labels = [1, 0, 1, 0]  # Corresponding labels: 1 = Correct, 0 = Incorrect

# Create test dataset
test_dataset = SinhalaGrammarDataset(test_sentences, test_labels, tokenizer, max_length=128)

# Evaluate the model on the test set
test_results = trainer.evaluate(test_dataset)

print("\nTest Results:")
print(test_results)

# Add prediction function
def predict(sentence, tokenizer, model):
    inputs = tokenizer.encode_plus(
        sentence,
        max_length=128,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # Set model to evaluation mode
    model.eval()

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        probabilities = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(probabilities).item()

    return "Correct" if predicted_class == 1 else "Incorrect"

# Test the model on example sentences
print("\nTesting Individual Sentences:")
for sentence in test_sentences:
    result = predict(sentence, tokenizer, model)
    print(f"Sentence: '{sentence}' => Prediction: {result}")